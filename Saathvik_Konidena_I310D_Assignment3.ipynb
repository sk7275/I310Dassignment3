{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "Sxm1seAcgvnb"
      },
      "outputs": [],
      "source": [
        "from googleapiclient import discovery\n",
        "import json\n",
        "\n",
        "API_KEY = 'AIzaSyCovcJ3jD3iLKNCrUZek2foOe6stM7zMi8'\n",
        "\n",
        "client = discovery.build(\n",
        "  \"commentanalyzer\",\n",
        "  \"v1alpha1\",\n",
        "  developerKey=API_KEY,\n",
        "  discoveryServiceUrl=\"https://commentanalyzer.googleapis.com/$discovery/rest?version=v1alpha1\",\n",
        "  static_discovery=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "My hypothesis is that the Perspective API will demonstrate increased accuracy in identifying toxic language within longer statements compared to shorter ones. The expectation is grounded in the belief that longer content provides the model with more text data, enabling it to better discern potentially toxic language. This assumption suggests that the model's performance may vary based on the length of the input text, as I anticipate a higher accuracy for lengthier statements."
      ],
      "metadata": {
        "id": "srKcrfaz7AZ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "testSet = [(\"bro screw you\", \"short\"), (\"hey you look like you're gaining a lot of weight fatty\",\"long\"),\n",
        "           (\"women are useless\",\"short\"), (\"why do you look like that today?\", \"long\"),\n",
        "           (\"you're too emotional\", \"short\"), (\"I think you're being a bit sensitive\", \"long\"),\n",
        "           (\"bro what the hell\", \"short\"), (\"Pretty good for a woman in your field.\",\"long\"),\n",
        "           (\"bad weather today\", \"short\"), (\"bro today is such a bad day\", \"long\"),\n",
        "           (\"bad job\", \"short\"), (\"you could improve a bit more\", \"long\"),\n",
        "           (\"hey chill man\", \"short\"), (\"watch what you say my guy, be nice\", \"long\"),\n",
        "           (\"watch your words\", \"short\"), (\"there are some things you should be careful about\", \"long\")]\n",
        "actualScores = [1,1,1,1,1,0,0,1,0,0,0,0,0,0,0,0,1,1]\n",
        "\n",
        "toxicLevel = 0.5\n",
        "scores = []\n",
        "\n",
        "for statement, length in testSet:\n",
        "  analyze_request = {\n",
        "  'comment': { 'text': statement },\n",
        "  'requestedAttributes': {'TOXICITY': {}}\n",
        "  }\n",
        "\n",
        "  response = client.comments().analyze(body=analyze_request).execute()\n",
        "  toxicity_score = response['attributeScores']['TOXICITY']['summaryScore']['value']\n",
        "  scores.append(toxicity_score)\n",
        "  #print(response)\n",
        "print(scores)\n",
        "\n",
        "predScores = []\n",
        "for score in scores:\n",
        "  if score > 0.5:\n",
        "    predScores.append(1)\n",
        "  else:\n",
        "    predScores.append(0)\n",
        "\n",
        "short_indices = []\n",
        "long_indices = []\n",
        "\n",
        "i = 0\n",
        "for statement, length in testSet:\n",
        "  if length == \"short\":\n",
        "    short_indices.append(i)\n",
        "    i = i+1\n",
        "  else:\n",
        "    long_indices.append(i)\n",
        "    i = i+1\n",
        "print(short_indices)\n",
        "print(long_indices)\n",
        "\n",
        "y_actual_short = [actualScores[i] for i in short_indices]\n",
        "y_predicted_short = [predScores[i] for i in short_indices]\n",
        "\n",
        "y_actual_long = [actualScores[i] for i in long_indices]\n",
        "y_predicted_long = [predScores[i] for i in long_indices]\n",
        "\n",
        "print(actualScores)\n",
        "print(predScores)"
      ],
      "metadata": {
        "id": "p2EVdLhKC6Ho",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b918426-ecba-4aaa-ee25-55f3d74261b2"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.71760553, 0.54325575, 0.7570315, 0.034041706, 0.1013248, 0.055768944, 0.47886392, 0.112643376, 0.045131154, 0.12520397, 0.22846605, 0.09741997, 0.03875561, 0.08716487, 0.049336795, 0.046120718]\n",
            "[0, 2, 4, 6, 8, 10, 12, 14]\n",
            "[1, 3, 5, 7, 9, 11, 13, 15]\n",
            "[1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]\n",
            "[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def class_wise_acc(actualScores, predScores):\n",
        "    total_p = 0\n",
        "    total_n = 0\n",
        "    TP=0\n",
        "    TN=0\n",
        "    for i in range(len(predScores)):\n",
        "        if actualScores[i]==1:\n",
        "            total_p = total_p+1\n",
        "            if actualScores[i]==predScores[i]:\n",
        "               TP=TP+1\n",
        "        if actualScores[i]==0:\n",
        "            total_n=total_n+1\n",
        "            if actualScores[i]==predScores[i]:\n",
        "               TN=TN+1\n",
        "    return(TP/total_p, TN/total_n)\n",
        "\n",
        "class_1_acc_short, class_0_acc_short = class_wise_acc(y_actual_short, y_predicted_short)\n",
        "class_1_acc_long, class_0_acc_long = class_wise_acc(y_actual_long, y_predicted_long)\n",
        "\n",
        "print (f\"Class 1 (i.e., >50K) accuracy for Short = {class_1_acc_short}\")\n",
        "print (f\"Class 0 (i.e., <50K) accuracy for Short = {class_0_acc_short}\")\n",
        "print (f\"Class 1 (i.e., >50K) accuracy for Long = {class_1_acc_long}\")\n",
        "print (f\"Class 0 (i.e., >50K) accuracy for Long = {class_0_acc_long}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdfV1FqFwbur",
        "outputId": "83cb87f7-1887-4577-ea94-781e493dd590"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class 1 (i.e., >50K) accuracy for Short = 0.6666666666666666\n",
            "Class 0 (i.e., <50K) accuracy for Short = 1.0\n",
            "Class 1 (i.e., >50K) accuracy for Long = 0.3333333333333333\n",
            "Class 0 (i.e., >50K) accuracy for Long = 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The findings of the analysis on the Perspective API's accuracy reveal an interesting and unexpected pattern. While the model demonstrated 100% accuracy in identifying non-toxic statements, its performance dropped significantly for toxic statements, particularly with shorter ones achieving 66% accuracy and longer ones only reaching 33%. These results suggest potential biases in the model, indicating a challenge in accurately identifying toxicity, especially in shorter content. It prompts a critical examination of the training data and the underlying assumptions of the model.\n",
        "\n",
        "The biases in the model may arise from the training data and the way toxic and non-toxic examples were presented during the model's development. If the training data was imbalanced, with a higher proportion of non-toxic longer statements compared to toxic longer statements, the model might overemphasize the non-toxic patterns in lengthy content, leading to the observed discrepancy in accuracy. Additionally, biases may exist in the model's understanding of toxicity in short statements, where contextual nuances and language intricacies may be more challenging to capture accurately. Public documentation about the Perspective API's training process and datasets could shed light on these potential biases and provide insights into the model's limitations.\n",
        "\n",
        "The unexpected results highlight the complexity of training models for toxicity detection and the importance of thorough evaluation. Theories about the model's lower accuracy for toxic statements, especially in shorter content, could revolve around the challenges in capturing the subtleties of toxic language in limited textual context. Shorter statements may lack the context necessary for the model to accurately discern between playful banter and genuinely offensive content. Additionally, biases in training data, particularly in representing toxic shorter statements, could contribute to the observed performance gap. Addressing these issues may require refining the training data, considering context-aware features, and continuous model evaluation and improvement to enhance the model's effectiveness in different content lengths and toxicity levels."
      ],
      "metadata": {
        "id": "dOpgz4QA6fq3"
      }
    }
  ]
}