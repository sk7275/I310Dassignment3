# I310Dassignment3
This is the repository for I310D assignment #3

The findings of the analysis on the Perspective API's accuracy reveal an interesting and unexpected pattern. While the model demonstrated 100% accuracy in identifying non-toxic statements, its performance dropped significantly for toxic statements, particularly with shorter ones achieving 66% accuracy and longer ones only reaching 33%. These results suggest potential biases in the model, indicating a challenge in accurately identifying toxicity, especially in shorter content. It prompts a critical examination of the training data and the underlying assumptions of the model.

The biases in the model may arise from the training data and the way toxic and non-toxic examples were presented during the model's development. If the training data was imbalanced, with a higher proportion of non-toxic longer statements compared to toxic longer statements, the model might overemphasize the non-toxic patterns in lengthy content, leading to the observed discrepancy in accuracy. Additionally, biases may exist in the model's understanding of toxicity in short statements, where contextual nuances and language intricacies may be more challenging to capture accurately. Public documentation about the Perspective API's training process and datasets could shed light on these potential biases and provide insights into the model's limitations.

The unexpected results highlight the complexity of training models for toxicity detection and the importance of thorough evaluation. Theories about the model's lower accuracy for toxic statements, especially in shorter content, could revolve around the challenges in capturing the subtleties of toxic language in limited textual context. Shorter statements may lack the context necessary for the model to accurately discern between playful banter and genuinely offensive content. Additionally, biases in training data, particularly in representing toxic shorter statements, could contribute to the observed performance gap. Addressing these issues may require refining the training data, considering context-aware features, and continuous model evaluation and improvement to enhance the model's effectiveness in different content lengths and toxicity levels.
